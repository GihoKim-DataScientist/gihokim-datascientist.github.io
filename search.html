<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- 웹 폰트 설정 -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css 추가 -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="주니어 개발자의 개발 노트 (Junior ML college students notes)" />
    <link rel="shortcut icon" href="https://gihokim-datascientist.github.io/assets/images/favicon.png" type="image/png" />
    <link rel="canonical" href="https://gihokim-datascientist.github.io/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Jr Data Scientist  Giho's Brain of Machine Learning" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="주니어 개발자의 개발 노트 (Junior ML college students notes)" />
    <meta property="og:url" content="https://gihokim-datascientist.github.io/search" />
    <meta property="og:image" content="https://gihokim-datascientist.github.io/assets/built/images/cover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="주니어 개발자의 개발 노트 (Junior ML college students notes)" />
    <meta name="twitter:url" content="https://gihokim-datascientist.github.io/" />
    <meta name="twitter:image" content="https://gihokim-datascientist.github.io/assets/built/images/cover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Jr Data Scientist  Giho's Brain of Machine Learning" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Jr Data Scientist  Giho's Brain of Machine Learning",
        "logo": "https://gihokim-datascientist.github.io/"
    },
    "url": "https://gihokim-datascientist.github.io/search",
    "image": {
        "@type": "ImageObject",
        "url": "https://gihokim-datascientist.github.io/assets/built/images/cover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://gihokim-datascientist.github.io/search"
    },
    "description": "주니어 개발자의 개발 노트 (Junior ML college students notes)"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://gihokim-datascientist.github.io/">Jr Data Scientist  Giho's Brain of Machine Learning</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-algorithms" role="menuitem"><a href="/tag/algorithms/">알고리즘 (Algorithms)</a></li>
    <li class="nav-modeling" role="menuitem"><a href="/tag/modeling/">Modeling 실습 예제</a></li>
    <li class="nav-codingquestions" role="menuitem"><a href="/tag/codingquestions/">Coding Questions</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tags' posts</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "k-means-clustering-code": {
        "title": "Machine Learning 실습예제 - k-means clustering",
            "author": "GihoKim",
            "category": "",
            "content": "Modeling 구성     Machine Learning 실습예제 (1) - linear regression    Machine Learning 실습예제 (2) - K-means clusteringK-Means ClusteringWe will use the following imports for this problemimport osimport numpy as npimport pandas as pdPart BImplement a Python function called cost that takes in a cluster and a centroid, and returns the cost of the cluster associated with that centroid. Assume that each point is given as a list of coordinates, and each cluster is given as a list of points. The five data points you worked with in part (a) are given in this format below.Hint: Recall that $cost(\\mu_j)$ is the total squared distance of each $\\overrightarrow{x_i}$ in $C_j$ to $\\mu_j$. Start by defining the helper function squared_distance.def squared_distance(x, u):    \"\"\"Function to calculate the square of the Euclidian distance between a data point x and its centroid u\"\"\"    return (x[0] - u[0]) ** 2 + (x[1] - u[1]) ** 2def cost(C, u):    \"\"\"Function to calulate the cost of a cluster and its centroid\"\"\"    cost = 0    for i in C:        distance = squared_distance(i, u)        cost = cost + distance    return costx1=[3,10]x2=[5,76]x3=[1,8]x4=[2,9]x5=[3,78]C1= [x1, x3, x4]C2= [x2, x5]Now, calculate the total cost before the first iteration and after the first iteration of the algorithm, with these five data points. Recall that the total cost $cost(\\mu_1, \\dots, \\mu_k)$ is defined as \\(cost(\\mu_1, \\dots, \\mu_k) = cost(\\mu_1)+\\dots+ cost(\\mu_k).\\)You should find that the cost has decreased after the first iteration.before_cost = cost(C1, x1) + cost(C2, x2)after_cost = cost(C1, [2, 9]) + cost(C2, [4, 77])print(\"The total cost before the first iteration is\", before_cost)print(\"The total cost after the first iteration is\", after_cost)The total cost before the first iteration is 18The total cost after the first iteration is 8Part CUsing the data points in k_means_data.csv, follow the prompts below to implement the k-means clustering algorithm with $k=3$ clusters.We’ll start by reading in the data.fp = os.path.join(\"k_means_data.csv\")table = pd.read_csv(fp) #Import the datadata = table.values.tolist() #Put the data in a form that is easier to useFirst, write a function to initialize the centroids by choosing $k$ random data points. We’ll use initialize_centroids with $k=3$ since we want to find three clusters.def initialize_centroids(data, k):    \"\"\"Return k randomly selected centroids from among the data points\"\"\"    random_numbers = np.random.choice(len(data), 3, replace = False)    return [data[random_numbers[0]], data[random_numbers[1]], data[random_numbers[2]]]centroids = initialize_centroids(data, 3)centroids[[2.4257491105903943, 7.9548128907273234], [6.705188928008579, 78.00202552760933], [3.9966062985857884, 77.1128640020023]]Next, implement a function that determines the closest centroid for a given data point. The function find_closest_centroid should take as input a single data point as well as a list of centroids. It should return the centroid that’s closest to the given data point.Hint: You can use the squared_distance function you’ve implemented above.def find_closest_centroid(point, centroids):    \"\"\"Function to determine the closest centroid to a given data point\"\"\"    lst = []    for i in centroids:        distance = squared_distance(point, i)        lst.append(distance)    minimum = min(lst)    index = lst.index(minimum)    return centroids[index]The function below puts each data point into a group (cluster), based on which of the centroids it is closest to. The groups are labeled 1 through $k$, where group 1 contains the points that are closest to the first centroid in the centroids list.def define_groups(data, centroids):       groups = []    for point in data:        closest = find_closest_centroid(point, centroids)               groups.append(centroids.index(closest)+1)    return groupsgroup_list = define_groups(data, centroids)Now, we need to relocate the centroids to the average of all points in that group. The function to do that is provided below. You should read through it and make sure you understand how it works.def update_centroids(data, k, group_list):    \"\"\"Function that takes in data, k, and a list of groups and returns a list of new centroids\"\"\"    new_centroids=[]    for i in np.arange(k)+1:               #for each cluster        total = np.array([0.0,0.0])        count = 0        for j in range(len(data)):          #go through each data point            if group_list[j]==i:                   #if the data point is in this cluster                total+=np.array(data[j])                          #add the data point to this cluster's total                count+=1                                          #and increment the count of points in this cluster        new_centroid = total/count        new_centroids.append(list(new_centroid))    return new_centroidsnew_centroids = update_centroids(data, 3, group_list)new_centroids[[-1.1198909887743935, 14.547028738295456], [6.097314819873166, 78.12828163677702], [4.4561307948150395, 77.47263629489235]]Finally, put together the functions above to run the k-means algorithm to find the centroids that yield the least cost. To make it a little easier, just update the centroids a fixed number of times, $1000$ times. You can think about how you would implement a better stopping condition.Complete the k_means function below. The inputs are the data, the value of $k$, and the initial centroids. The function should return a list of the centroids after $1000$ updates.def k_means(data, k, initial_centroids):    \"\"\"Implement the k-means algorithm given data points, number of clusters, and initial centroids as input.    Return a list of centroids with least cost.\"\"\"    group_list = define_groups(data, initial_centroids)    for i in range(1000):        updated_centriods = update_centroids(data, k, group_list)        group_list = define_groups(data, updated_centriods)    return updated_centriodsk_means(data, 3, initialize_centroids(data, 3))[[1.9920820677628333, 9.118216804679005], [5.028058561123173, 77.7011187625188], [-4.231864045311622, 19.9758406719119]]For this part, turn in screenshots of the three functions you wrote:  initialize_centroids  find_closest_centroid  k_meansas well as the output of your k_means function.x_values = []y_values = []for i in data:    x_values.append(i[0])    y_values.append(i[1])data_df = {    'X': x_values,    'Y': y_values}import pandas as pdimport matplotlib.pyplot as pltdf = pd.DataFrame(data_df, columns = ['X', 'Y'])df                  X      Y                  0      0.671723      11.737969              1      -3.757326      18.536079              2      5.275413      78.514845              3      4.006203      77.254795              4      -5.764720      20.356741              ...      ...      ...              193      4.050378      77.779269              194      -3.449263      18.837490              195      3.740859      76.031105              196      -5.839885      17.661508              197      1.513687      10.920654      198 rows × 2 columnscentroids_1000 = k_means(data, 3, initialize_centroids(data, 3))centroids_1000[[-4.231864045311622, 19.9758406719119], [5.028058561123173, 77.7011187625188], [1.9920820677628333, 9.118216804679005]]plt.plot(df['X'], df['Y'], '.')plt.plot(centroids_1000[0][0], centroids_1000[0][1], 'bo')plt.plot(centroids_1000[1][0], centroids_1000[1][1], 'ro')plt.plot(centroids_1000[2][0], centroids_1000[2][1], 'ko')[&lt;matplotlib.lines.Line2D at 0x190a2d63e80&gt;]​  ",
        "url": "/k-means-clustering-code"
    }
    ,
    
    "linear-regression-modeling": {
        "title": "Machine Learning 실습예제 - linear regression",
            "author": "GihoKim",
            "category": "",
            "content": "Modeling 구성     Machine Learning 실습예제 (1) - linear regression    Machine Learning 실습예제 (2) - K-means clusteringimport pandas as pdimport matplotlib.pyplot as pltimport statsmodels# Problem 4: Climate data modeling# We will load data from the Univ. of East Anglia Climate Research Unit (CRU) # (https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.03/).# The following variables are available (see: https://climatedataguide.ucar.edu/climate-data/cru-ts-gridded-precipitation-and-other-meteorological-variables-1901#:~:text=The%20CRU%20TS%20series%20of,0%20is%20a%20recent%20release))# tmp - Temperature# cld - Cloud cover# dtr - Diurnal temperature range# frs - Ground frost frequency# pet - potential evapotranspiration# pre - precipitation# vap - vapor pressure# wet - Wet day frequency# This is the base URL where the data are stored:base_url = 'https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.05/crucy.2103081329.v4.05/countries/VAR/crucy.v4.05.1901.2020.all.VAR.per'# Load the temperature data. The field \"ANN\" contains the annual average.url = base_url.replace('VAR','tmp')data_tmp = pd.read_fwf(url, skiprows=3) # Read \"fixed-width format\"data_tmp = data_tmp[['YEAR','ANN']]data_tmp.head()                  YEAR      ANN                  0      1901      13.0              1      1902      12.8              2      1903      12.8              3      1904      12.8              4      1905      12.9      # Now loop through each of the variables and load each one.# Append them to each other to create a joint dataframe containing# all of the desired variables.# Make a list of the variables we want to examine:variables = ['tmp','wet','frs','cld']data_list = []for var in variables:    url = base_url.replace('VAR',var)    data_var = pd.read_fwf(url, skiprows=3) # Read \"fixed-width format\"    data_var = data_var[['YEAR','ANN']]        # Rename the \"ANN\" column (annual)    data_var = data_var.rename(columns={'ANN':var})        # Set \"YEAR\" to be the index    data_var = data_var.set_index('YEAR')    data_list.append(data_var)data = data_list[0].join(data_list[1:]).reset_index()data.head()                  YEAR      tmp      wet      frs      cld                  0      1901      13.0      110.5      99.4      54.7              1      1902      12.8      110.1      101.6      54.6              2      1903      12.8      111.0      101.2      54.9              3      1904      12.8      110.7      100.9      54.7              4      1905      12.9      111.1      101.3      55.0      # Make a plot showing each of the variablesdata.plot(x='YEAR',subplots=True)array([&lt;AxesSubplot:xlabel='YEAR'&gt;, &lt;AxesSubplot:xlabel='YEAR'&gt;,       &lt;AxesSubplot:xlabel='YEAR'&gt;, &lt;AxesSubplot:xlabel='YEAR'&gt;],      dtype=object)from statsmodels.formula.api import ols?olsplt.plot(data[\"YEAR\"],data[\"tmp\"], '.')plt.xlabel(\"Time in Year\")plt.ylabel(\"Celcius\")Text(0, 0.5, 'Celcius')4.(a)They appear correlated posivitly which means temperature increases as time goes.Their relationship is linear.# 4.bmodel = ols(formula='tmp ~ 1+ YEAR',data = data)# Fit the model:model = model.fit()print(model.summary())                            OLS Regression Results                            ==============================================================================Dep. Variable:                    tmp   R-squared:                       0.665Model:                            OLS   Adj. R-squared:                  0.662Method:                 Least Squares   F-statistic:                     234.1Date:                Tue, 27 Apr 2021   Prob (F-statistic):           8.65e-30Time:                        05:23:26   Log-Likelihood:                -2.7541No. Observations:                 120   AIC:                             9.508Df Residuals:                     118   BIC:                             15.08Df Model:                           1                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P&gt;|t|      [0.025      0.975]------------------------------------------------------------------------------Intercept     -6.4968      1.290     -5.036      0.000      -9.052      -3.942YEAR           0.0101      0.001     15.301      0.000       0.009       0.011==============================================================================Omnibus:                        1.569   Durbin-Watson:                   0.732Prob(Omnibus):                  0.456   Jarque-Bera (JB):                1.551Skew:                          -0.269   Prob(JB):                        0.460Kurtosis:                       2.856   Cond. No.                     1.11e+05==============================================================================Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.11e+05. This might indicate that there arestrong multicollinearity or other numerical problems.estimated intercept = coefficient of InterceptThis is given in the model summary which is -6.4968estimated slop = coefficient of YEARThis is given in the model summary which is 0.0101data[\"predict\"] = model.predict()plt.plot(data[\"YEAR\"],data[\"tmp\"], '.')plt.plot(data[\"YEAR\"], data[\"predict\"])plt.xlabel(\"YEAR\")plt.ylabel(\"tmp\")Text(0, 0.5, 'tmp')Yes, There a statistically significant effect of x4 on y since the regression line shows that positive slope which is y value increases when x value increasesmodel.pvaluesIntercept    1.730160e-06YEAR         8.653127e-30dtype: float644.cThe 95% confidence interval for the slope of x4 is between 0.009 and 0.011 which is shown in model summarymodel_cld = ols(formula='tmp ~ 1 + cld',data = data)# Fit the model:model_cld = model_cld.fit()print(model_cld.summary())                            OLS Regression Results                            ==============================================================================Dep. Variable:                    tmp   R-squared:                       0.076Model:                            OLS   Adj. R-squared:                  0.068Method:                 Least Squares   F-statistic:                     9.731Date:                Tue, 27 Apr 2021   Prob (F-statistic):            0.00228Time:                        05:23:26   Log-Likelihood:                -63.599No. Observations:                 120   AIC:                             131.2Df Residuals:                     118   BIC:                             136.8Df Model:                           1                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P&gt;|t|      [0.025      0.975]------------------------------------------------------------------------------Intercept     -5.2952      5.942     -0.891      0.375     -17.063       6.472cld            0.3366      0.108      3.120      0.002       0.123       0.550==============================================================================Omnibus:                       17.828   Durbin-Watson:                   0.284Prob(Omnibus):                  0.000   Jarque-Bera (JB):               21.160Skew:                           1.016   Prob(JB):                     2.54e-05Kurtosis:                       3.320   Cond. No.                     8.65e+03==============================================================================Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 8.65e+03. This might indicate that there arestrong multicollinearity or other numerical problems.estimated intercept = coefficient of InterceptThis is given in the model summary which is -5.2952estimated slop = coefficient of YEARThis is given in the model summary which is 0.3366data[\"predict_cld\"] = model_cld.predict()plt.plot(data[\"cld\"],data[\"tmp\"], '.')plt.plot(data[\"cld\"], data[\"predict_cld\"])plt.xlabel(\"cld\")plt.ylabel(\"tmp\")Text(0, 0.5, 'tmp')model_cld.pvaluesIntercept    0.374691cld          0.002277dtype: float64There is a statistically significant effect of x3 on y since the p-value of model_cld is less than 0.05 which means we reject the null-hypothes.4.eformula is y = 0.3366x1 - 5.2952Then the question ask the change in tmp if cld increases by 0.2 so 0.3366 * 0.2 = 0.06732I would predict 0.06732 increases in tmp if cld increases by 0.2.model_multiple = ols(formula='tmp ~ 1 + wet + frs + cld',data = data)# Fit the model:model_multiple = model_multiple.fit()print(model_multiple.summary())                            OLS Regression Results                            ==============================================================================Dep. Variable:                    tmp   R-squared:                       0.943Model:                            OLS   Adj. R-squared:                  0.941Method:                 Least Squares   F-statistic:                     637.5Date:                Tue, 13 Apr 2021   Prob (F-statistic):           7.04e-72Time:                        11:29:14   Log-Likelihood:                 103.33No. Observations:                 120   AIC:                            -198.7Df Residuals:                     116   BIC:                            -187.5Df Model:                           3                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P&gt;|t|      [0.025      0.975]------------------------------------------------------------------------------Intercept     28.5361      1.753     16.277      0.000      25.064      32.008wet           -0.0267      0.007     -3.573      0.001      -0.042      -0.012frs           -0.1464      0.004    -37.902      0.000      -0.154      -0.139cld            0.0355      0.035      1.026      0.307      -0.033       0.104==============================================================================Omnibus:                        0.615   Durbin-Watson:                   1.917Prob(Omnibus):                  0.735   Jarque-Bera (JB):                0.727Skew:                          -0.057   Prob(JB):                        0.695Kurtosis:                       2.636   Cond. No.                     2.91e+04==============================================================================Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.91e+04. This might indicate that there arestrong multicollinearity or other numerical problems.model_multiple.pvaluesIntercept    9.866650e-32wet          5.159167e-04frs          3.491024e-67cld          3.068349e-01dtype: float644.fSince the p-values of wet and frs are less than 0.05 which means that they reject null-hypothesis. Thus, wet and frs are the parameters which have a statistically significant effect.4.gThe single linear regression has no effect of other parameters since it has only one parameter. However, multiple linear regression has multiple parameters which can effect each other.4.hdata[\"predict_multiple\"] = model_multiple.predict()plt.plot(data[\"tmp\"],data[\"predict_multiple\"], '.')[&lt;matplotlib.lines.Line2D at 0x7f33416d83d0&gt;]from scipy.stats import pearsonrcorr = pearsonr(data['tmp'], data['predict_multiple'])corr[0]0.97098535716220374.iThe fraction of the varianc of y that is captured by the model is 0.943 as I can see on the model summary",
        "url": "/linear-regression-modeling"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://gihokim-datascientist.github.io/">Jr Data Scientist  Giho's Brain of Machine Learning</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search for posts</h1>
            <p class="subscribe-overlay-description">
                 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="keyword" />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>

        </div>
    </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-15FG7HRMTM', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
