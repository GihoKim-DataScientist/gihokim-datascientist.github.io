<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- 웹 폰트 설정 -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css 추가 -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="주니어 데이터 사이언티스트 김기호의 컴퓨터 공부 일기" />
    <link rel="shortcut icon" href="https://gihokim-datascientist.github.io/assets/images/favicon.png" type="image/png" />
    <link rel="canonical" href="https://gihokim-datascientist.github.io/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Giho's Data Science Notes" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="주니어 데이터 사이언티스트 김기호의 컴퓨터 공부 일기" />
    <meta property="og:url" content="https://gihokim-datascientist.github.io/search" />
    <meta property="og:image" content="https://gihokim-datascientist.github.io/assets/built/images/maincover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="주니어 데이터 사이언티스트 김기호의 컴퓨터 공부 일기" />
    <meta name="twitter:url" content="https://gihokim-datascientist.github.io/" />
    <meta name="twitter:image" content="https://gihokim-datascientist.github.io/assets/built/images/maincover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Giho's Data Science Notes" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Giho's Data Science Notes",
        "logo": "https://gihokim-datascientist.github.io/"
    },
    "url": "https://gihokim-datascientist.github.io/search",
    "image": {
        "@type": "ImageObject",
        "url": "https://gihokim-datascientist.github.io/assets/built/images/maincover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://gihokim-datascientist.github.io/search"
    },
    "description": "주니어 데이터 사이언티스트 김기호의 컴퓨터 공부 일기"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://gihokim-datascientist.github.io/">Giho's Data Science Notes</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-algorithms" role="menuitem"><a href="/tag/algorithms/">알고리즘 (Algorithms)</a></li>
    <li class="nav-modeling" role="menuitem"><a href="/tag/modeling/">Modeling 실습 예제</a></li>
    <li class="nav-codingquestions" role="menuitem"><a href="/tag/codingquestions/">Coding Questions</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tags' posts</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "example": {
        "title": "Coding Questions (1) - 카카오톡 여름 인턴 문제 1번",
            "author": "GihoKim",
            "category": "",
            "content": "Coding Questions 구성     Coding Questiosn (1) - 카카오톡 2021 여름 인턴 코딩 문제 1번import pandas as pdiphone_df = pd.read_csv('desktop/codeit/iphone.csv', index_col = 0)iphone_df                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone 7      2016-09-16      4.7      2GB      iOS 10.0      No              iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0      No              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      iphone_df.loc['iPhone 8', '메모리'] #iphone_df.loc[row, column]'2GB'# iphone_df.loc['iPhone 8', '가격'] 가격이라는 column이 존재하지않음iphone_df.loc['iPhone X', :] #모든 columns출시일        2017-11-03디스플레이             5.8메모리               3GB출시 버전        iOS 11.1Face ID           YesName: iPhone X, dtype: objecttype(iphone_df.loc['iPhone X'])pandas.core.series.Seriesiphone_df.loc[:, '출시일']iPhone 7         2016-09-16iPhone 7 Plus    2016-09-16iPhone 8         2017-09-22iPhone 8 Plus    2017-09-22iPhone X         2017-11-03iPhone XS        2018-09-21iPhone XS Max    2018-09-21Name: 출시일, dtype: objectiphone_df['디스플레이'] #column부를때는 .loc따로 필요없음iPhone 7         4.7iPhone 7 Plus    5.5iPhone 8         4.7iPhone 8 Plus    5.5iPhone X         5.8iPhone XS        5.8iPhone XS Max    6.5Name: 디스플레이, dtype: float64iphone_df                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone 7      2016-09-16      4.7      2GB      iOS 10.0      No              iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0      No              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      #row 불러오기iphone_df.loc['iPhone X'] #return pandas series출시일        2017-11-03디스플레이             5.8메모리               3GB출시 버전        iOS 11.1Face ID           YesName: iPhone X, dtype: objectiphone_df.loc[['iPhone X', 'iPhone 8']]                   출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No      type(iphone_df.loc[['iPhone X', 'iPhone 8']]) #return pandas dataFramepandas.core.frame.DataFrame#columns 불러오기iphone_df[['Face ID', '출시일', '메모리']]                  Face ID      출시일      메모리                  iPhone 7      No      2016-09-16      2GB              iPhone 7 Plus      No      2016-09-16      3GB              iPhone 8      No      2017-09-22      2GB              iPhone 8 Plus      No      2017-09-22      3GB              iPhone X      Yes      2017-11-03      3GB              iPhone XS      Yes      2018-09-21      4GB              iPhone XS Max      Yes      2018-09-21      4GB      #슬라이싱iphone_df.loc['iPhone 8': 'iPhone XS']                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes      #trickyiphone_df['메모리': 'Face ID']                  출시일      디스플레이      메모리      출시 버전      Face ID          #column slicingiphone_df.loc[:, \"메모리\": \"Face ID\"]                  메모리      출시 버전      Face ID                  iPhone 7      2GB      iOS 10.0      No              iPhone 7 Plus      3GB      iOS 10.0      No              iPhone 8      2GB      iOS 11.0      No              iPhone 8 Plus      3GB      iOS 11.0      No              iPhone X      3GB      iOS 11.1      Yes              iPhone XS      4GB      iOS 12.0      Yes              iPhone XS Max      4GB      iOS 12.0      Yes      #with boolean for rowiphone_df.loc[[True, False, True, True, False, True, False]]                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone 7      2016-09-16      4.7      2GB      iOS 10.0      No              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes      #with boolean for columniphone_df.loc[:, [True, False, False, True, False]]                  출시일      출시 버전                  iPhone 7      2016-09-16      iOS 10.0              iPhone 7 Plus      2016-09-16      iOS 10.0              iPhone 8      2017-09-22      iOS 11.0              iPhone 8 Plus      2017-09-22      iOS 11.0              iPhone X      2017-11-03      iOS 11.1              iPhone XS      2018-09-21      iOS 12.0              iPhone XS Max      2018-09-21      iOS 12.0      iphone_df['디스플레이'] &gt; 5iPhone 7         FalseiPhone 7 Plus     TrueiPhone 8         FalseiPhone 8 Plus     TrueiPhone X          TrueiPhone XS         TrueiPhone XS Max     TrueName: 디스플레이, dtype: booliphone_df.loc[iphone_df['디스플레이'] &gt; 5]                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0      No              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      iphone_df['Face ID'] == 'Yes'iPhone 7         FalseiPhone 7 Plus    FalseiPhone 8         FalseiPhone 8 Plus    FalseiPhone X          TrueiPhone XS         TrueiPhone XS Max     TrueName: Face ID, dtype: booliphone_df.loc[iphone_df['Face ID'] == 'Yes']                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      condition = (iphone_df['Face ID'] == 'Yes') &amp; (iphone_df['디스플레이'] &gt; 5)#2가지 묶을때는 괄호 쓰기iphone_df.loc[condition]                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      iphone_df                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone 7      2016-09-16      4.7      2GB      iOS 10.0      No              iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0      No              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      #DataFrame 위치로 인덱싱하기iphone_df.iloc[2, 4] #iloc[row, column]'No'iphone_df.iloc[[1,3], [1,4]]                  디스플레이      Face ID                  iPhone 7 Plus      5.5      No              iPhone 8 Plus      5.5      No      iphone_df.iloc[3:, 1:4]                  디스플레이      메모리      출시 버전                  iPhone 8 Plus      5.5      3GB      iOS 11.0              iPhone X      5.8      3GB      iOS 11.1              iPhone XS      5.8      4GB      iOS 12.0              iPhone XS Max      6.5      4GB      iOS 12.0      iphone_df                  출시일      디스플레이      메모리      출시 버전      Face ID                  iPhone 7      2016-09-16      4.7      2GB      iOS 10.0      No              iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0      No              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      iphone_df.loc['iPhone 7'] #return list of rowiphone_df.loc[['iPhone 7', 'iPhone 7 Plus']] #return dataFrame of rows#same func row slicingiphone_df.loc['iPhone 7' : 'iPhone X']iphone_df['iPhone 7' : 'iPhone X']#same func to get columniphone_df.loc[:, '출시일']iphone_df['출시일']#same func to get specific columnsiphone_df.loc[:, ['출시일', '디스플레이']]iphone_df[['출시일', '디스플레이']]#column slicingiphone_df.loc[:, \"출시일\":\"출시 버전\"]                  출시일      디스플레이      메모리      출시 버전                  iPhone 7      2016-09-16      4.7      2GB      iOS 10.0              iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0              iPhone X      2017-11-03      5.8      3GB      iOS 11.1              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      #one row locationiphone_df.iloc[0]#multiple row locationsiphone_df.iloc[[1, 2, 3]]#row slicingiphone_df.iloc[1: 3]iphone_df[1:3]#one column locationiphone_df.iloc[:, 3]#multiple column locationsiphone_df.iloc[:, [1, 2, 3]]#column slicingiphone_df.iloc[:, 0:3]                  출시일      디스플레이      메모리                  iPhone 7      2016-09-16      4.7      2GB              iPhone 7 Plus      2016-09-16      5.5      3GB              iPhone 8      2017-09-22      4.7      2GB              iPhone 8 Plus      2017-09-22      5.5      3GB              iPhone X      2017-11-03      5.8      3GB              iPhone XS      2018-09-21      5.8      4GB              iPhone XS Max      2018-09-21      6.5      4GB      iphone_df                  출시일      디스플레이      메모리      출시 버전      Face ID      확인용                  iPhone 7      1      1.0      1      1      1      100              iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0      No      80              iPhone 8      1      1.0      1      1      1      81              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No      82              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes      50              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes      40              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      30      iphone_df['디스플레이'] &lt; 5iPhone 7          TrueiPhone 7 Plus    FalseiPhone 8          TrueiPhone 8 Plus    FalseiPhone X         FalseiPhone XS        FalseiPhone XS Max    FalseName: 디스플레이, dtype: booliphone_df                  출시일      디스플레이      메모리      출시 버전      Face ID      확인용                  iPhone 7      2016-09-16      4.7      2GB      iOS 10.0      No      100              iPhone 7 Plus      2016-09-16      5.5      3GB      iOS 10.0      No      80              iPhone 8      2017-09-22      4.7      2GB      iOS 11.0      No      81              iPhone 8 Plus      2017-09-22      5.5      3GB      iOS 11.0      No      82              iPhone X      2017-11-03      5.8      3GB      iOS 11.1      Yes      50              iPhone XS      2018-09-21      5.8      4GB      iOS 12.0      Yes      40              iPhone XS Max      2018-09-21      6.5      4GB      iOS 12.0      Yes      30      iphone_df[['디스플레이', '확인용']] &gt; 5                  디스플레이      확인용                  iPhone 7      False      True              iPhone 7 Plus      True      True              iPhone 8      False      True              iPhone 8 Plus      True      True              iPhone X      True      True              iPhone XS      True      True              iPhone XS Max      True      True      iphone_df.loc[:, ['디스플레이', '확인용']] &gt; 5                  디스플레이      확인용                  iPhone 7      False      True              iPhone 7 Plus      True      True              iPhone 8      False      True              iPhone 8 Plus      True      True              iPhone X      True      True              iPhone XS      True      True              iPhone XS Max      True      True      ",
        "url": "/example"
    }
    ,
    
    "k-means-clustering-code": {
        "title": "Machine Learning 실습예제 - k-means clustering",
            "author": "GihoKim",
            "category": "",
            "content": "Modeling 구성     Machine Learning 실습예제 (1) - linear regression    Machine Learning 실습예제 (2) - K-means clusteringK-Means ClusteringWe will use the following imports for this problemimport osimport numpy as npimport pandas as pdPart BImplement a Python function called cost that takes in a cluster and a centroid, and returns the cost of the cluster associated with that centroid. Assume that each point is given as a list of coordinates, and each cluster is given as a list of points. The five data points you worked with in part (a) are given in this format below.Hint: Recall that $cost(\\mu_j)$ is the total squared distance of each $\\overrightarrow{x_i}$ in $C_j$ to $\\mu_j$. Start by defining the helper function squared_distance.def squared_distance(x, u):    \"\"\"Function to calculate the square of the Euclidian distance between a data point x and its centroid u\"\"\"    return (x[0] - u[0]) ** 2 + (x[1] - u[1]) ** 2def cost(C, u):    \"\"\"Function to calulate the cost of a cluster and its centroid\"\"\"    cost = 0    for i in C:        distance = squared_distance(i, u)        cost = cost + distance    return costx1=[3,10]x2=[5,76]x3=[1,8]x4=[2,9]x5=[3,78]C1= [x1, x3, x4]C2= [x2, x5]Now, calculate the total cost before the first iteration and after the first iteration of the algorithm, with these five data points. Recall that the total cost $cost(\\mu_1, \\dots, \\mu_k)$ is defined as \\(cost(\\mu_1, \\dots, \\mu_k) = cost(\\mu_1)+\\dots+ cost(\\mu_k).\\)You should find that the cost has decreased after the first iteration.before_cost = cost(C1, x1) + cost(C2, x2)after_cost = cost(C1, [2, 9]) + cost(C2, [4, 77])print(\"The total cost before the first iteration is\", before_cost)print(\"The total cost after the first iteration is\", after_cost)The total cost before the first iteration is 18The total cost after the first iteration is 8Part CUsing the data points in k_means_data.csv, follow the prompts below to implement the k-means clustering algorithm with $k=3$ clusters.We’ll start by reading in the data.fp = os.path.join(\"k_means_data.csv\")table = pd.read_csv(fp) #Import the datadata = table.values.tolist() #Put the data in a form that is easier to useFirst, write a function to initialize the centroids by choosing $k$ random data points. We’ll use initialize_centroids with $k=3$ since we want to find three clusters.def initialize_centroids(data, k):    \"\"\"Return k randomly selected centroids from among the data points\"\"\"    random_numbers = np.random.choice(len(data), 3, replace = False)    return [data[random_numbers[0]], data[random_numbers[1]], data[random_numbers[2]]]centroids = initialize_centroids(data, 3)centroids[[2.4257491105903943, 7.9548128907273234], [6.705188928008579, 78.00202552760933], [3.9966062985857884, 77.1128640020023]]Next, implement a function that determines the closest centroid for a given data point. The function find_closest_centroid should take as input a single data point as well as a list of centroids. It should return the centroid that’s closest to the given data point.Hint: You can use the squared_distance function you’ve implemented above.def find_closest_centroid(point, centroids):    \"\"\"Function to determine the closest centroid to a given data point\"\"\"    lst = []    for i in centroids:        distance = squared_distance(point, i)        lst.append(distance)    minimum = min(lst)    index = lst.index(minimum)    return centroids[index]The function below puts each data point into a group (cluster), based on which of the centroids it is closest to. The groups are labeled 1 through $k$, where group 1 contains the points that are closest to the first centroid in the centroids list.def define_groups(data, centroids):       groups = []    for point in data:        closest = find_closest_centroid(point, centroids)               groups.append(centroids.index(closest)+1)    return groupsgroup_list = define_groups(data, centroids)Now, we need to relocate the centroids to the average of all points in that group. The function to do that is provided below. You should read through it and make sure you understand how it works.def update_centroids(data, k, group_list):    \"\"\"Function that takes in data, k, and a list of groups and returns a list of new centroids\"\"\"    new_centroids=[]    for i in np.arange(k)+1:               #for each cluster        total = np.array([0.0,0.0])        count = 0        for j in range(len(data)):          #go through each data point            if group_list[j]==i:                   #if the data point is in this cluster                total+=np.array(data[j])                          #add the data point to this cluster's total                count+=1                                          #and increment the count of points in this cluster        new_centroid = total/count        new_centroids.append(list(new_centroid))    return new_centroidsnew_centroids = update_centroids(data, 3, group_list)new_centroids[[-1.1198909887743935, 14.547028738295456], [6.097314819873166, 78.12828163677702], [4.4561307948150395, 77.47263629489235]]Finally, put together the functions above to run the k-means algorithm to find the centroids that yield the least cost. To make it a little easier, just update the centroids a fixed number of times, $1000$ times. You can think about how you would implement a better stopping condition.Complete the k_means function below. The inputs are the data, the value of $k$, and the initial centroids. The function should return a list of the centroids after $1000$ updates.def k_means(data, k, initial_centroids):    \"\"\"Implement the k-means algorithm given data points, number of clusters, and initial centroids as input.    Return a list of centroids with least cost.\"\"\"    group_list = define_groups(data, initial_centroids)    for i in range(1000):        updated_centriods = update_centroids(data, k, group_list)        group_list = define_groups(data, updated_centriods)    return updated_centriodsk_means(data, 3, initialize_centroids(data, 3))[[1.9920820677628333, 9.118216804679005], [5.028058561123173, 77.7011187625188], [-4.231864045311622, 19.9758406719119]]For this part, turn in screenshots of the three functions you wrote:  initialize_centroids  find_closest_centroid  k_meansas well as the output of your k_means function.x_values = []y_values = []for i in data:    x_values.append(i[0])    y_values.append(i[1])data_df = {    'X': x_values,    'Y': y_values}import pandas as pdimport matplotlib.pyplot as pltdf = pd.DataFrame(data_df, columns = ['X', 'Y'])df                  X      Y                  0      0.671723      11.737969              1      -3.757326      18.536079              2      5.275413      78.514845              3      4.006203      77.254795              4      -5.764720      20.356741              ...      ...      ...              193      4.050378      77.779269              194      -3.449263      18.837490              195      3.740859      76.031105              196      -5.839885      17.661508              197      1.513687      10.920654      198 rows × 2 columnscentroids_1000 = k_means(data, 3, initialize_centroids(data, 3))centroids_1000[[-4.231864045311622, 19.9758406719119], [5.028058561123173, 77.7011187625188], [1.9920820677628333, 9.118216804679005]]plt.plot(df['X'], df['Y'], '.')plt.plot(centroids_1000[0][0], centroids_1000[0][1], 'bo')plt.plot(centroids_1000[1][0], centroids_1000[1][1], 'ro')plt.plot(centroids_1000[2][0], centroids_1000[2][1], 'ko')[&lt;matplotlib.lines.Line2D at 0x190a2d63e80&gt;]",
        "url": "/k-means-clustering-code"
    }
    ,
    
    "algorithm-logistic-regression-1": {
        "title": "Machine Learning Algorithm - logistic regression (1)",
            "author": "GihoKim",
            "category": "",
            "content": "Algorithms 구성     Algorithms (1) - Machine learning basic 기초    Algorithms (2) - Machine learning linear regression (1)    Algorithms (2) - Machine learning linear regression (2)    Algorithms (2) - Machine learning linear regression (3)    Algorithms (2) - Machine learning linear regression (4)    Algorithms (3) - Machine learning logistic regression (1)Logistic regression (classification)  y (output)가 범주형 data 일때 사용 (categorical y)  분류예측을 하는 모델링          제품이 불량인지 분류      코로나가 걸렸는지 안걸렸는지 등        parameter들을 가지고 y값을 predict 하여 분류하는 modelBernoulli random variableYi = B0 + B1 Xi + ei; Yi = 0 or 1  assume E(ei) = 0 -&gt; Yi = B0 + B1 Xi          P(Yi = 1) = prob_x 라고 가정할때      P(Yi = 0) = 1 - prob_x 로 나타낼수 있다        E(Yi) = 1 * prob_x + 0 * (1 - prob_x) = prob_x  E(Yi) = B0 + B1 Xi = prob_xx 값이 주어졌을 때 y값이  1을 가질 확률로 표현하는 모델링이런 data를 linear regression으로 fitting하기에는 적합하지 않다.  나이별로 그룹을 만들고 질병을 보유하고 있는 확률을 계산  그래프로 표시하면 빨간색 선이 파란색 linear regression보다 더 정확한 line이다.이런 관계를 나타낼수 있는 함수가 Logistic 함수 혹은 sigmoid 함수라고 한다.  항상 f(x)는 0과 1 사이임parameter x가 범주 1에 속할 확률이 logistic function  E(y) = P(X = x) = P(Y = 1 given X = x) = 1 - P(Y = 0 given X = x)이렇게도 표현이 가능하다  “logistic” functionwhere z =  B0 + B1Xparameter B0 과 B1이 y값과 linear relationship이 아니라서 직관적인 해석이 안된다그래서 odd 개념을 접목시켜야된다odds (승산)성공확률을 p라고 정의할때 실패 대비 성공 확률 비율  odd = p / (1 -p)ex) 월드컵에서 프랑스가 이길 odd가 2 / 11이라면 p/(1 - p)가 2/11이므로 p로 풀어내면 p = 2/13인 0.15 (15%)가 된다odds의 개념을 logistic에 접목을 시키면 이런식으로 표현이 가능하다odd에다가 log를 취하면 단순환 선형 결합의 형태로 바뀐다. (logit transformation)여기서는 B1에 대한 해석이 직관적으로 바뀜 (x가 증가하면 log(odd)가 증가함; x가 증가함에 따라 성공확률이 log scale로 얼마만큼 증가했다라고 할 수 있다)",
        "url": "/algorithm-logistic-regression-1"
    }
    ,
    
    "linear-regression-model": {
        "title": "Machine Learning 실습예제 - linear regression",
            "author": "GihoKim",
            "category": "",
            "content": "Modeling 구성     Machine Learning 실습예제 (1) - linear regression    Machine Learning 실습예제 (2) - K-means clusteringLinear regression modelingimport pandas as pdimport matplotlib.pyplot as pltimport statsmodels# load data from the Univ. of East Anglia Climate Research Unit (CRU) # (https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.03/).# The following variables are available# tmp - Temperature# cld - Cloud cover# dtr - Diurnal temperature range# frs - Ground frost frequency# pet - potential evapotranspiration# pre - precipitation# vap - vapor pressure# wet - Wet day frequency# This is the base URL where the data are stored:base_url = 'https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.05/crucy.2103081329.v4.05/countries/VAR/crucy.v4.05.1901.2020.all.VAR.per'# Load the temperature data. The field \"ANN\" contains the annual average.url = base_url.replace('VAR','tmp')data_tmp = pd.read_fwf(url, skiprows=3) # Read \"fixed-width format\"data_tmp = data_tmp[['YEAR','ANN']]data_tmp.head()# 어떤식으로 작동하는지는 모름                  YEAR      ANN                  0      1901      13.0              1      1902      12.8              2      1903      12.8              3      1904      12.8              4      1905      12.9      # Now loop through each of the variables and load each one.# Append them to each other to create a joint dataframe containing# all of the desired variables.# Make a list of the variables we want to examine:variables = ['tmp','wet','frs','cld']data_list = []for var in variables:    url = base_url.replace('VAR',var)    data_var = pd.read_fwf(url, skiprows=3) # Read \"fixed-width format\"    data_var = data_var[['YEAR','ANN']]        # Rename the \"ANN\" column (annual)    data_var = data_var.rename(columns={'ANN':var})        # Set \"YEAR\" to be the index    data_var = data_var.set_index('YEAR')    data_list.append(data_var)data = data_list[0].join(data_list[1:]).reset_index()data.head()# 어떤식으로 작동하는지는 모름                  YEAR      tmp      wet      frs      cld                  0      1901      13.0      110.5      99.4      54.7              1      1902      12.8      110.1      101.6      54.6              2      1903      12.8      111.0      101.2      54.9              3      1904      12.8      110.7      100.9      54.7              4      1905      12.9      111.1      101.3      55.0      # 각각의 column들을 x값에 year를 넣고 plotdata.plot(x='YEAR',subplots=True)array([&lt;AxesSubplot:xlabel='YEAR'&gt;, &lt;AxesSubplot:xlabel='YEAR'&gt;,       &lt;AxesSubplot:xlabel='YEAR'&gt;, &lt;AxesSubplot:xlabel='YEAR'&gt;],      dtype=object)# ols (lilnear regression) importfrom statsmodels.formula.api import ols# 매해 온도 변화 plot으로 나타냄plt.plot(data[\"YEAR\"],data[\"tmp\"], '.')plt.xlabel(\"Time in Year\")plt.ylabel(\"Celcius\")Text(0, 0.5, 'Celcius')They appear correlated posivitly which means temperature increases as time goes.Their relationship is linear.매년 온도가 증가하는 것으로 보이므로 x 변수 year와 y 변수 temperature의 관계가 linear regression (선형 회귀)로 보임# ols function을 사용해서 model을 만듬model = ols(formula='tmp ~ 1+ YEAR',data = data)# Fit the modelmodel = model.fit()print(model.summary())                            OLS Regression Results                            ==============================================================================Dep. Variable:                    tmp   R-squared:                       0.665Model:                            OLS   Adj. R-squared:                  0.662Method:                 Least Squares   F-statistic:                     234.1Date:                Thu, 13 May 2021   Prob (F-statistic):           8.65e-30Time:                        09:27:23   Log-Likelihood:                -2.7541No. Observations:                 120   AIC:                             9.508Df Residuals:                     118   BIC:                             15.08Df Model:                           1                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P&gt;|t|      [0.025      0.975]------------------------------------------------------------------------------Intercept     -6.4968      1.290     -5.036      0.000      -9.052      -3.942YEAR           0.0101      0.001     15.301      0.000       0.009       0.011==============================================================================Omnibus:                        1.569   Durbin-Watson:                   0.732Prob(Omnibus):                  0.456   Jarque-Bera (JB):                1.551Skew:                          -0.269   Prob(JB):                        0.460Kurtosis:                       2.856   Cond. No.                     1.11e+05==============================================================================Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.11e+05. This might indicate that there arestrong multicollinearity or other numerical problems.point estimate of intercept: -6.4968point estimate of ‘year’: 0.0101# 모델링의 lilne을 기존 scatter plot에 표시data[\"predict\"] = model.predict()plt.plot(data[\"YEAR\"],data[\"tmp\"], '.')plt.plot(data[\"YEAR\"], data[\"predict\"])plt.xlabel(\"YEAR\")plt.ylabel(\"tmp\")Text(0, 0.5, 'tmp')모델의 라인을 보면 x value인 year가 증가할수록 y value인 온도도 증가하는 것으로 보임. 기울기가 positive인것으로 보아 매년 지구의 온도가 올라감.model.pvaluesIntercept    1.730160e-06YEAR         8.653127e-30dtype: float64p value가 0에 가깝기 때문에 year는 statistically significant한 변수임# 지구 온도와 구름의 상관관계를 나타내는 모델model_cld = ols(formula='tmp ~ 1 + cld',data = data)# Fit the model:model_cld = model_cld.fit()print(model_cld.summary())                            OLS Regression Results                            ==============================================================================Dep. Variable:                    tmp   R-squared:                       0.076Model:                            OLS   Adj. R-squared:                  0.068Method:                 Least Squares   F-statistic:                     9.731Date:                Thu, 13 May 2021   Prob (F-statistic):            0.00228Time:                        09:35:30   Log-Likelihood:                -63.599No. Observations:                 120   AIC:                             131.2Df Residuals:                     118   BIC:                             136.8Df Model:                           1                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P&gt;|t|      [0.025      0.975]------------------------------------------------------------------------------Intercept     -5.2952      5.942     -0.891      0.375     -17.063       6.472cld            0.3366      0.108      3.120      0.002       0.123       0.550==============================================================================Omnibus:                       17.828   Durbin-Watson:                   0.284Prob(Omnibus):                  0.000   Jarque-Bera (JB):               21.160Skew:                           1.016   Prob(JB):                     2.54e-05Kurtosis:                       3.320   Cond. No.                     8.65e+03==============================================================================Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 8.65e+03. This might indicate that there arestrong multicollinearity or other numerical problems.point estimate of intercept: -5.2952point estimate of ‘cld’: 0.3366#  모델링의 lilne을 기존 scatter plot에 표시data[\"predict_cld\"] = model_cld.predict()plt.plot(data[\"cld\"],data[\"tmp\"], '.')plt.plot(data[\"cld\"], data[\"predict_cld\"])plt.xlabel(\"cld\")plt.ylabel(\"tmp\")Text(0, 0.5, 'tmp')model_cld.pvaluesIntercept    0.374691cld          0.002277dtype: float64cld 변수의 p value가 0.5보다 크지 않으므로 statistically significant한 변수라고 할 수 있음.식으로 나타내면 formula: y = 0.3366x1 - 5.2952 표현할수 있음# 지구 온도와 여러가지 parameters과의 상관관계를 나타내는 모델model_multiple = ols(formula='tmp ~ 1 + wet + frs + cld',data = data)# Fit the model:model_multiple = model_multiple.fit()print(model_multiple.summary())                            OLS Regression Results                            ==============================================================================Dep. Variable:                    tmp   R-squared:                       0.943Model:                            OLS   Adj. R-squared:                  0.941Method:                 Least Squares   F-statistic:                     637.5Date:                Thu, 13 May 2021   Prob (F-statistic):           7.04e-72Time:                        09:38:08   Log-Likelihood:                 103.33No. Observations:                 120   AIC:                            -198.7Df Residuals:                     116   BIC:                            -187.5Df Model:                           3                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P&gt;|t|      [0.025      0.975]------------------------------------------------------------------------------Intercept     28.5361      1.753     16.277      0.000      25.064      32.008wet           -0.0267      0.007     -3.573      0.001      -0.042      -0.012frs           -0.1464      0.004    -37.902      0.000      -0.154      -0.139cld            0.0355      0.035      1.026      0.307      -0.033       0.104==============================================================================Omnibus:                        0.615   Durbin-Watson:                   1.917Prob(Omnibus):                  0.735   Jarque-Bera (JB):                0.727Skew:                          -0.057   Prob(JB):                        0.695Kurtosis:                       2.636   Cond. No.                     2.91e+04==============================================================================Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.91e+04. This might indicate that there arestrong multicollinearity or other numerical problems.model_multiple.pvaluesIntercept    9.866650e-32wet          5.159167e-04frs          3.491024e-67cld          3.068349e-01dtype: float64모든 parameters의 p-value가 0에 가까우므로 reject null hypothesis와 모든 parameters는 statistically significant한 변수들이다.multiple linear regression으로 모델링 한것과 sigle linear regression으로 모델링한것의 point estimate가 다른 것으로 보아 각각의 parameters들은 서로 영향을 끼치는 것을 알 수 있다.# 모델의 predictiondata[\"predict_multiple\"] = model_multiple.predict()plt.plot(data[\"tmp\"],data[\"predict_multiple\"], '.')[&lt;matplotlib.lines.Line2D at 0x195cf567190&gt;]from scipy.stats import pearsonrcorr = pearsonr(data['tmp'], data['predict_multiple'])corr(0.9709853571622034, 3.6172955865432555e-75)# 상관 계수 pearson r (correlation coefficient)corr[0]0.9709853571622034correlation coefficient가 양수 인것으로 보아 각각의 parameter들은 서로 linear한 관계를 가지고 있다 (ex. x1이 증가하면 x2도 증가함)  fraction of variance of y : R^2  R^2가 0.943 (in the summary of model)이므로 변수들의 설명력이 94.3% 이고 y에 대해well-described된 parameters를 가진 모델로 설명할 수 있다",
        "url": "/linear-regression-model"
    }
    ,
    
    "algorithm-linear-regression-4": {
        "title": "Machine Learning Algorithm - linear regression (4)",
            "author": "GihoKim",
            "category": "",
            "content": "Algorithms 구성     Algorithms (1) - Machine learning basic 기초    Algorithms (2) - Machine learning linear regression (1)    Algorithms (2) - Machine learning linear regression (2)    Algorithms (2) - Machine learning linear regression (3)    Algorithms (2) - Machine learning linear regression (4)    Algorithms (3) - Machine learning logistic regression (1)결정계수 (coefficient of determination: R^2)  Yi = 실제 Y 값  두번째 점은 모델링에서 나온 ^Y값  Y bar은 실제 Y값의 평균값R^2  SSE = 실제 Y값과 모델의 ^Y 값들의 차이의 제곱 합 (Residual sum of squares와 같은 맥락)  SST = 모델의 ^Y값과 실제 Y의 평균값과의 차이의 제곱 합  SST = RSS + ESSRSS / TSS = R^2  1인 경우: SST = SSR + (SSE = 0); model의 직선이 실제 Y값들을 지나감 (확정적인 관계)현재 가지고 있는 x변수로 y를 100% 설명, 모든 y가 linear regression line위에 있다  0인 경우: SST = (SSR = 0) + SSE; model의 직선이 Y값의 평균을 지나감 (parameter x가 아무런 y에 효과가 없음)현재 가지고 있는 x변수는 y를 설명(예측)에 전혀 도움이 되지않는다  사용하고 있는 x변수가 y변수의 variance를 얼마나 줄였는지 정도  단순히 y의 평균값을 사용했을 때 대비 x정보를 사용함으로써 얻는 성능 향상 정도수정 결정계수 (Adjusted R^2)  R^2는 의미없는 변수 x가 추가되어도 항상 증가한다  수정 R^2는 의미없는 변수 x가 추가되었을때 R^2가 증가하는것을 방지하기위해 특정 계수를 식앞에 곱해줌  의미있는 변수 x가 추가되었을때는 SSE값이 떨어지기 때문에 전체 adj R^2는 증가하게 된다예제  parameter x1 (판매원 수)와 x2 (광고비) 에 의해 y (매출액)의 variance가 68.3% 감소  매출액의 평균 (y bar)에 대비 x1, x2 parameter를 이용하면 설명력이 68.3% 증가  현재 분석에 사용하고 있는 판매원 수와 광고비는 “변수 품질”정도가 68.3 (100점 기준)분산 분석 (analysis of variance)RSS / ESS (x 변수에 의해 설명된 것 / 에러에 의한 설명된 것)  SSR / SSE &gt; 1          RSS &gt; ESS; x변수가 y변수에 statistically significant      x변수의 기울기가 0이 아님 -&gt; reject null hypothesis        0 &lt;= SSR / SSE &lt;= 1          SSR &lt; SSE; x변수가 y변수에 not statistically significant      x변수의 기울기가 0 -&gt; null hypothesis is true      ",
        "url": "/algorithm-linear-regression-4"
    }
    ,
    
    "algorithm-linear-regression-3": {
        "title": "Machine Learning Algorithm - linear regression (3)",
            "author": "GihoKim",
            "category": "",
            "content": "Algorithms 구성     Algorithms (1) - Machine learning basic 기초    Algorithms (2) - Machine learning linear regression (1)    Algorithms (2) - Machine learning linear regression (2)    Algorithms (2) - Machine learning linear regression (3)    Algorithms (2) - Machine learning linear regression (4)    Algorithms (3) - Machine learning logistic regression (1)Parameter Estimation AlgorithmsLeast squared estimator  estimator: a function of the samples  purpose of estimator: to find out unknown parameters  types of estimator          point estimator (점 추정)      interval estimator (구간 추정)      point estimatorYi = w0 + w1 Xi + εi, εi ~ N(0, σ²), i = 1, 2, … n  w0 에 대한 point estimator: ^w0 = mean(y) - ^w1 * mean(x)  w1 에 대한 point estimator: linear regression 2 참조  σ²에 대한 point estimator: ^σ²= (1 / n - 2) sum ( ei²) -&gt; residualInterval Estimation  구간으로 측정하여 보다 유연한 정보 제공  θ (parameter)에 대한 구간 측정 -&gt; ^θ - 상수값 * std(θ) &lt;= θ &lt;= ^θ + 상수값 * std(θ)기울기에 대한 가설검정  unknown parameter에 대한 가설을 세우고 이를 검정p-value  p-value가 0.05나 0.01보다 작으면 reject null hypothesisstatsmodels.formula.api에 ols function을 통한 모델링을 했을때  parameters: wet, frs, cld, intercept  point estimates: 각각의 coef  std: 각각의 std err  t: 전반적으로 모든 parameters들의 abs(t)값이 &gt; 1.96 (p &lt; 0.05) 또는 abs(t) &gt; 2.58 (p &lt; 0.01) 큼 -&gt; null hypothesis를 reject  p-value: 모든 slope parameters이 0.05보다 큰 값이 없기 때문에 statistically significant",
        "url": "/algorithm-linear-regression-3"
    }
    ,
    
    "algorithm-linear-regression-2": {
        "title": "Machine Learning Algorithm - linear regression (2)",
            "author": "GihoKim",
            "category": "",
            "content": "Algorithms 구성     Algorithms (1) - Machine learning basic 기초    Algorithms (2) - Machine learning linear regression (1)    Algorithms (2) - Machine learning linear regression (2)    Algorithms (2) - Machine learning linear regression (3)    Algorithms (2) - Machine learning linear regression (4)    Algorithms (3) - Machine learning logistic regression (1)Parameter Estimate Algorithms  mean squared errorfunction H(x)를 w0 + w1 X1으로 두었기 때문에이런식으로 바뀌게 되는것을 알 수 있다.To find best fit line  cost function을 MSE로 썻기 때문에 parabola 형태를 뛴 그래프 형태가 될 것이다  gradient descent를 적용 w0과 w1에 대한 partial derivative를 각각 구한다  partial derivative = 0 꼴로 만들고 w0과 w1으로 풀어쓰게 되면 w0과 w1에 대한 식이 생겨남^Y = ^w0 + ^w1 X 꼴의 식이 성립된다Least squared Estimation Algorithm (최소제곱법)Residual (잔차)  e = y - ^y 최소 제곱법으로 구해진 직선의 ^Y와 실제 Y값의 차이 (정해진 값)  ε = y - E(y) 오차항은 확률 분포를 따름  잔차 e는 확률 오차 ε이 실제로 구현된 값",
        "url": "/algorithm-linear-regression-2"
    }
    ,
    
    "algorithm-linear-regression-1": {
        "title": "Machine Learning Algorithm - linear regression (1)",
            "author": "GihoKim",
            "category": "",
            "content": "Algorithms 구성     Algorithms (1) - Machine learning basic 기초    Algorithms (2) - Machine learning linear regression (1)    Algorithms (2) - Machine learning linear regression (2)    Algorithms (2) - Machine learning linear regression (3)    Algorithms (2) - Machine learning linear regression (4)    Algorithms (3) - Machine learning logistic regression (1)Linear regression (선형 회귀)변수 사이의 관계  확정적 관계x 변수만으로 y를 100% 표현 (오차 없음)ex) force = f(mass, acceleration), distance = f(velocity, time)  확률적 관계ex) 포도주 가격 = f(강우량, 온도, 포도품종) + εFrancis Galtonrelationship of height between father and son선형 회귀 모델 (linear regression)선형회귀 모델: 출력변수 Y를 입력변수 x들의 선형 결합으로 표현한 모델선형결합: 변수들을 (상수 배와) 더하기 빼기를 통해 결합      Y = B + B1 X1 + B2 X2 + ... Bp Xp  x 변수가 한개 인 경우: Y = B0 + B1 X (직선식)B0 = intercept (절편), B1 = slope (기울기)purpose of linear regression  to interpret the relationship between x and y  to predict the output value yRegression model types선형회귀는 가장 데이터에 알맞은 함수 (line)를 찾는 것(Find the best fit line of different lines)",
        "url": "/algorithm-linear-regression-1"
    }
    ,
    
    "algorithms-basic": {
        "title": "Machine Learning Algorithm - 기초 Basic",
            "author": "GihoKim",
            "category": "",
            "content": "Algorithms 구성     Algorithms (1) - Machine learning basic 기초    Algorithms (2) - Machine learning linear regression (1)    Algorithms (2) - Machine learning linear regression (2)    Algorithms (2) - Machine learning linear regression (3)    Algorithms (2) - Machine learning linear regression (4)    Algorithms (3) - Machine learning logistic regression (1)모델링이란?      주어진 독립변수, 예측변수, input 변수 (predictor variables) x        주어진 종속변수 반응변수 output 변수 (response variables) y 를 가지고        알맞은 함수식 (hypothesis function or prediction rule) function f(x) 를 찾는것  Training data and Testing data      Training data (학습 데이터)는 모델 f(x)구축시 사용되는 데이터        Testing data (검증 데이터)는 구축된 모델을 검증하는데 사용되는 데이터  AI란?Artifical intelligence는 어느 기계에 함수가 들어가는 것 (세탁기에 함수가 들어가면 인공지능 세탁기가 된다!)모델링의 종류      linear regression (선형 회귀)        logistic regression (로지스틱 회귀)        decision tree (의사결정나무)        random forest (랜덤 포레스트)        network (네트워크 모델 #신경망모델)  데이터y의 종류  연송형 (quantitative) : 숫자로 표현 (가격, 길이, 압력, 두께)  범주형 (categorical): 숫자로 표현 안되는 데이터 (제품 불량 여부, 보험 사기 여부)regression (수치 예측)  큰 의미의 수치 예측classification (범주 예측, 분류)  0 = 불량, 1 = 정상regression example모델을 만들고 x값을 넣었을때 numerical y값을 구하는 모델classification example두 종류를 나눠주는 모델을 만들고 데이터가 주어졌을때 그 것이 어느 종류인지 알려주는 모델features  (특성들)을 바탕으로 불량품인지 아닌지 또는 정상인지 비정상인지 ‘종류’를 예측 하는 모델      x 와 y 의 관계를 찾자        y를 설명하는 x 변수는 보통 여러개 이다        여러 종류의 x를 가지고 y와의 관계를 찾는 것        Y = f(X1, X2, … Xp)  Parameters (매개 변수)coefficient = parameters (include intercept)Y = w1 X1 + w2 X2 + ε      여기서는 w1 과 w2가 parameter, ε은 오차 (error)        모델의 parameter를 찾는 것이 궁극적인 목표  Loss function = Y - f(x) (손실함수 = 오차를 구해주는 식)Best model은 Loss function이 적은 것 Y - f(x) = 0, ε = 0Cost function = Sum ( Yi - (w1 X1i + w2 X2i ) ) ^2개별적인 차이를 정의하는 식 // loss function과 비슷한 맥락Cost function을 최소로하는 parameters (w1, w2)를 찾자^w1, ^w2 (hat)Model’s parameter를 찾는 것이 핵심  how? : throughout given data  for what? : to make my prediction to be same as my actual data as much as possible",
        "url": "/algorithms-basic"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://gihokim-datascientist.github.io/">Giho's Data Science Notes</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search for posts</h1>
            <p class="subscribe-overlay-description">
                 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="keyword" />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>

        </div>
    </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-15FG7HRMTM', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
